{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"--- [MODEL COMPARISON NOTEBOOK] ---\")\n",
        "\n",
        "# --- 1. Configuration & Setup\n",
        "PROJECT_ROOT = os.getcwd()\n",
        "\n",
        "# --- Define paths to your four fine-tuned models and test files---\n",
        "MODEL_A_PATH = os.path.join(PROJECT_ROOT, \"models\", \"ethio-ner-xlm-roberta-base\", \"best-model\")\n",
        "MODEL_B_PATH = os.path.join(PROJECT_ROOT, \"models\", \"ethio-ner-distilbert-base-multilingual-cased\", \"best-model\")\n",
        "MODEL_C_PATH = os.path.join(PROJECT_ROOT, \"models\", \"ethio-ner-bert-tiny-amharic\", \"best-model\")\n",
        "MODEL_D_PATH = os.path.join(PROJECT_ROOT, \"models\", \"ethio-ner-afroxlmr-large-ner-masakhaner-1.0_2.0\", \"best-model\")\n",
        "TEST_DATA_PATH = os.path.join(PROJECT_ROOT, \"data\", \"data_splits\", \"test.conll\")\n",
        "\n",
        "print(f\"[*] Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"\\n[*] Model A (Generalist): {MODEL_A_PATH}\")\n",
        "print(f\"[*] Model B (Speed Focus): {MODEL_B_PATH}\")\n",
        "print(f\"[*] Model C (Monolingual Specialist): {MODEL_C_PATH}\")\n",
        "print(f\"[*] Model D (NER Specialist): {MODEL_D_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNwQksFRkcWT",
        "outputId": "f09db325-e042-4768-bd53-313c20a4d711"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [MODEL COMPARISON NOTEBOOK] ---\n",
            "[*] Project Root: /content/EthioMart-Amharic-NER-Project\n",
            "\n",
            "[*] Model A (Generalist): /content/EthioMart-Amharic-NER-Project/models/ethio-ner-xlm-roberta-base/best-model\n",
            "[*] Model B (Speed Focus): /content/EthioMart-Amharic-NER-Project/models/ethio-ner-distilbert-base-multilingual-cased/best-model\n",
            "[*] Model C (Monolingual Specialist): /content/EthioMart-Amharic-NER-Project/models/ethio-ner-bert-tiny-amharic/best-model\n",
            "[*] Model D (NER Specialist): /content/EthioMart-Amharic-NER-Project/models/ethio-ner-afroxlmr-large-ner-masakhaner-1.0_2.0/best-model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[*] Loading NER pipelines...\")\n",
        "# Use a try-except block to handle environments with/without a GPU\n",
        "try:\n",
        "    pipe_a = pipeline(\"ner\", model=MODEL_A_PATH, device=0, aggregation_strategy=\"simple\")\n",
        "    pipe_b = pipeline(\"ner\", model=MODEL_B_PATH, device=0, aggregation_strategy=\"simple\")\n",
        "    pipe_c = pipeline(\"ner\", model=MODEL_C_PATH, device=0, aggregation_strategy=\"simple\")\n",
        "    pipe_d = pipeline(\"ner\", model=MODEL_D_PATH, device=0, aggregation_strategy=\"simple\")\n",
        "    print(\"[+] Pipelines loaded successfully onto GPU.\")\n",
        "except Exception as e:\n",
        "    print(f\"[!] GPU not available or error occurred: {e}. Loading models on CPU.\")\n",
        "    pipe_a = pipeline(\"ner\", model=MODEL_A_PATH, aggregation_strategy=\"simple\")\n",
        "    pipe_b = pipeline(\"ner\", model=MODEL_B_PATH, aggregation_strategy=\"simple\")\n",
        "    pipe_c = pipeline(\"ner\", model=MODEL_C_PATH, aggregation_strategy=\"simple\")\n",
        "    pipe_d = pipeline(\"ner\", model=MODEL_D_PATH, aggregation_strategy=\"simple\")\n",
        "\n",
        "# Load the raw text sentences from the test file for the speed benchmark\n",
        "def load_test_sentences(file_path):\n",
        "    \"\"\"Loads just the text of sentences from a CoNLL file.\"\"\"\n",
        "    sentences = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        current_sentence = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \" \" in line:\n",
        "                current_sentence.append(line.split()[0])\n",
        "            elif current_sentence:\n",
        "                sentences.append(\" \".join(current_sentence))\n",
        "                current_sentence = []\n",
        "        if current_sentence: # Add the last sentence if file doesn't end with newline\n",
        "             sentences.append(\" \".join(current_sentence))\n",
        "    return sentences\n",
        "\n",
        "test_sentences = load_test_sentences(TEST_DATA_PATH)\n",
        "if not test_sentences:\n",
        "    raise ValueError(\"No sentences found in the test file. Cannot run speed test.\")\n",
        "\n",
        "print(f\"\\n[*] Loaded {len(test_sentences)} sentences for speed benchmark.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSW8egOokgjz",
        "outputId": "82c260cc-d1c6-49ec-fc8c-7d11f0e0dc82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[*] Loading NER pipelines...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Pipelines loaded successfully onto GPU.\n",
            "\n",
            "[*] Loaded 3 sentences for speed benchmark.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run Inference Speed Benchmark ---\n",
        "def benchmark_model(pipe, model_name):\n",
        "    \"\"\"Measures the inference speed of a given pipeline.\"\"\"\n",
        "    print(f\"\\n[*] Benchmarking {model_name}...\")\n",
        "    start_time = time.time()\n",
        "    for sentence in test_sentences:\n",
        "        pipe(sentence)\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    avg_time = total_time / len(test_sentences)\n",
        "    print(f\"[+] {model_name} completed in {total_time:.4f}s. Average: {avg_time:.4f}s/sentence.\")\n",
        "    return avg_time\n",
        "\n",
        "benchmark_model(pipe_a, \"Model A (XLM-Roberta)\")\n",
        "benchmark_model(pipe_b, \"Model B (DistilBERT)\")\n",
        "benchmark_model(pipe_c, \"Model C (Amharic-BERT)\")\n",
        "benchmark_model(pipe_d, \"Model D (MasakhaNER-Large)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM0QnUgPko9e",
        "outputId": "0d336e2c-a511-486d-e3a6-1fbccba0e299"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[*] Benchmarking Model A (XLM-Roberta)...\n",
            "[+] Model A (XLM-Roberta) completed in 0.5034s. Average: 0.1678s/sentence.\n",
            "\n",
            "[*] Benchmarking Model B (DistilBERT)...\n",
            "[+] Model B (DistilBERT) completed in 0.0381s. Average: 0.0127s/sentence.\n",
            "\n",
            "[*] Benchmarking Model C (Amharic-BERT)...\n",
            "[+] Model C (Amharic-BERT) completed in 0.0269s. Average: 0.0090s/sentence.\n",
            "\n",
            "[*] Benchmarking Model D (MasakhaNER-Large)...\n",
            "[+] Model D (MasakhaNER-Large) completed in 0.0941s. Average: 0.0314s/sentence.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03138240178426107"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}
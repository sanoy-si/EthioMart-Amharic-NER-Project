{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)\n",
    "\n",
    "MODEL_A_OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"models\", \"ethio-ner-xlm-roberta-base\")\n",
    "MODEL_B_OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"models\", \"ethio-ner-afro-xlm-roberta-base\")\n",
    "\n",
    "MODEL_A_PATH = os.path.join(MODEL_A_OUTPUT_DIR, \"best-model\")\n",
    "MODEL_B_PATH = os.path.join(MODEL_B_OUTPUT_DIR, \"best-model\")\n",
    "\n",
    "TEST_DATA_PATH = os.path.join(PROJECT_ROOT, \"data\", \"data_splits\", \"test.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- [MODEL COMPARISON] ---\")\n",
    "print(f\"[*] Model A: {MODEL_A_PATH}\")\n",
    "print(f\"[*] Model B: {MODEL_B_PATH}\")\n",
    "print(\"--------------------------\\n\")\n",
    "\n",
    "\n",
    "print(\"[*] Loading NER pipelines...\")\n",
    "# Use a try-except block to handle environments with/without a GPU\n",
    "try:\n",
    "    pipe_a = pipeline(\"ner\", model=MODEL_A_PATH, device=0, aggregation_strategy=\"simple\")\n",
    "    pipe_b = pipeline(\"ner\", model=MODEL_B_PATH, device=0, aggregation_strategy=\"simple\")\n",
    "    print(\"[+] Pipelines loaded successfully onto GPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"[!] GPU not available or error occurred: {e}. Loading models on CPU.\")\n",
    "    pipe_a = pipeline(\"ner\", model=MODEL_A_PATH, aggregation_strategy=\"simple\")\n",
    "    pipe_b = pipeline(\"ner\", model=MODEL_B_PATH, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_sentences(file_path):\n",
    "    \"\"\"Loads just the text of sentences from a CoNLL file.\"\"\"\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        current_sentence = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and \" \" in line:\n",
    "                current_sentence.append(line.split()[0])\n",
    "            elif current_sentence:\n",
    "                sentences.append(\" \".join(current_sentence))\n",
    "                current_sentence = []\n",
    "        if current_sentence: \n",
    "             sentences.append(\" \".join(current_sentence))\n",
    "    return sentences\n",
    "\n",
    "test_sentences = load_test_sentences(TEST_DATA_PATH)\n",
    "if not test_sentences:\n",
    "    raise ValueError(\"No sentences found in the test file. Cannot run speed test.\")\n",
    "\n",
    "print(f\"\\n[*] Loaded {len(test_sentences)} sentences for speed benchmark.\")\n",
    "\n",
    "# Inference Speed Benchmark ---\n",
    "print(\"[*] Benchmarking Model A (xlm-roberta-base)...\")\n",
    "start_time_a = time.time()\n",
    "for sentence in test_sentences:\n",
    "    pipe_a(sentence)\n",
    "end_time_a = time.time()\n",
    "total_time_a = end_time_a - start_time_a\n",
    "avg_time_a = total_time_a / len(test_sentences)\n",
    "\n",
    "print(f\"[*] Benchmarking Model B (afro-xlm-roberta-base)...\")\n",
    "start_time_b = time.time()\n",
    "for sentence in test_sentences:\n",
    "    pipe_b(sentence)\n",
    "end_time_b = time.time()\n",
    "total_time_b = end_time_b - start_time_b\n",
    "avg_time_b = total_time_b / len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda790e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[*] Assembling comparison data...\")\n",
    "metrics_a = {\"f1\": 0.0, \"precision\": 0.0, \"recall\": 0.0} # For xlm-roberta-base\n",
    "metrics_b = {\"f1\": 0.0, \"precision\": 0.0, \"recall\": 0.0} # For afro-xlm-roberta-base\n",
    "\n",
    "comparison_data = {\n",
    "    \"Model\": [\"XLM-Roberta-Base (Generalist)\", \"Afro-XLM-R-Base (Specialist)\"],\n",
    "    \"F1-Score (Test Set)\": [metrics_a['f1'], metrics_b['f1']],\n",
    "    \"Precision\": [metrics_a['precision'], metrics_b['precision']],\n",
    "    \"Recall\": [metrics_a['recall'], metrics_b['recall']],\n",
    "    \"Avg. Inference Time (s/sentence)\": [avg_time_a, avg_time_b]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Format the output for better viewing\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"\\n\\n--- EthioMart NER Model Comparison ---\")\n",
    "print(df_comparison.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
